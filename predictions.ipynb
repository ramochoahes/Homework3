{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classifier comparison\n",
    "\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and pre-processing data\n",
    "\n",
    "To begin with, I replaced NA's with zeros. \n",
    "\n",
    "Later, I will see how many features were mostly \"NA\". If they are not important, I will probably drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "trainX = [] \n",
    "with open(\"/home/lu/Documents/School/81_machine_learning/HW3/HW3/trainingData.txt\", 'r') as rp: \n",
    "    for line in rp.read().split(\"\\n\"): \n",
    "        row = line.split(\"\\t\") \n",
    "        i = 0 \n",
    "        for cell in row: \n",
    "            try: \n",
    "                row[i] = float(cell)\n",
    "            except ValueError: \n",
    "                row[i] = 0 \n",
    "            i+=1 \n",
    "        trainX.append(row)\n",
    "        \n",
    "trainY = [] \n",
    "with open(\"/home/lu/Documents/School/81_machine_learning/HW3/HW3/trainingTruth.txt\", 'r') as rp: \n",
    "    for line in rp.read().split(\"\\n\"): \n",
    "        trainY.append(line)\n",
    "        \n",
    "testX = [] \n",
    "with open(\"/home/lu/Documents/School/81_machine_learning/HW3/HW3/testData.txt\", 'r') as rp: \n",
    "    for line in rp.read().split(\"\\n\"): \n",
    "        row = line.split(\"\\t\") \n",
    "        i = 0 \n",
    "        for cell in row: \n",
    "            try: \n",
    "                row[i] = float(cell)\n",
    "            except ValueError: \n",
    "                row[i] = 0 \n",
    "            i+=1 \n",
    "        testX.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Split data into training and tests sets \n",
    "arrayX = np.array(trainX[0:17377])\n",
    "arrayY = np.array(trainY[0:17377])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(arrayX, arrayY, test_size=0.4, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier | Score on Test | Score on Train \n",
      "--- | --- | --- | \n",
      "DecisionTree |0.465|0.512\n",
      "['0.137', '0.155', '0.606', '0.102', 3]\n",
      "['0.065', '0.298', '0.394', '0.242', 3]\n",
      "['0.093', '0.250', '0.520', '0.138', 3]\n",
      "['0.188', '0.400', '0.203', '0.209', 2]\n",
      "['0.179', '0.307', '0.368', '0.146', 3]\n",
      "['0.270', '0.392', '0.032', '0.307', 2]\n",
      "['0.137', '0.155', '0.606', '0.102', 3]\n",
      "['0.137', '0.155', '0.606', '0.102', 3]\n",
      "['0.188', '0.400', '0.203', '0.209', 2]\n",
      "['0.313', '0.238', '0.327', '0.122', 3]\n",
      "QDA |0.659|1.000\n",
      "['0.000', '0.000', '1.000', '0.000', 3]\n",
      "['0.000', '0.000', '1.000', '0.000', 3]\n",
      "['0.000', '0.000', '1.000', '0.000', 3]\n",
      "['1.000', '0.000', '0.000', '0.000', 1]\n",
      "['0.000', '0.000', '1.000', '0.000', 3]\n",
      "['1.000', '0.000', '0.000', '0.000', 1]\n",
      "['0.000', '0.000', '0.000', '1.000', 4]\n",
      "['0.111', '0.889', '0.000', '0.000', 2]\n",
      "['0.001', '0.999', '0.000', '0.000', 2]\n",
      "['0.000', '0.000', '1.000', '0.000', 3]\n",
      "Rand Forest |0.606|0.772\n",
      "['0.276', '0.200', '0.415', '0.109', 3]\n",
      "['0.143', '0.219', '0.507', '0.130', 3]\n",
      "['0.129', '0.165', '0.577', '0.129', 3]\n",
      "['0.497', '0.157', '0.194', '0.152', 1]\n",
      "['0.101', '0.224', '0.542', '0.132', 3]\n",
      "['0.524', '0.230', '0.138', '0.108', 1]\n",
      "['0.229', '0.165', '0.350', '0.256', 3]\n",
      "['0.161', '0.205', '0.504', '0.131', 3]\n",
      "['0.246', '0.245', '0.274', '0.235', 3]\n",
      "['0.104', '0.232', '0.503', '0.162', 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifiers = {\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(3),\n",
    "    \"Linear SVC\": SVC(kernel=\"linear\", C=0.5, probability=True),\n",
    "    \"RBF SVM\": SVC(gamma=2, C=1, probability=True),\n",
    "    \"DecisionTree_old\": DecisionTreeClassifier(max_depth=5), \n",
    "    \"RandomForest_old\": RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=20, random_state=0),\n",
    "    \"Rand Forest\": RandomForestClassifier(n_estimators=30, criterion='entropy', max_depth=20, min_samples_leaf=20, \n",
    "                           bootstrap=True, oob_score=False, random_state=0 ),\n",
    "    \"Adaboost\": AdaBoostClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"Logit\": LogisticRegression(C=1), \n",
    "    \"Logit l=2\": LogisticRegression(C=0.5), \n",
    "    \"Logit l=4\": LogisticRegression(C=0.25),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "print(\"Classifier | Score on Test | Score on Train \")\n",
    "print(\"--- | --- | --- | \")\n",
    "\n",
    "\n",
    "import operator\n",
    "for name, classifier in  classifiers.items() : \n",
    "    clf=classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    ## Sanity check by predicting what is in the trainingTruth.txt file \n",
    "    prob_array = clf.predict_proba(arrayX[0:10])\n",
    "    row = [\n",
    "        clf.score(X_test, y_test) ,clf.score(X_train, y_train), #comparing test and training on 4/6 split\n",
    "         ]\n",
    "    print( \"%s |\"%name + \"|\".join([\"%.3f\" % x for x in row]) )\n",
    "    \n",
    "    for row in prob_array: \n",
    "        index, value = max(enumerate(row), key=operator.itemgetter(1))\n",
    "        print([\"%.3f\" % x for x in row] + [index+1] )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Predict using the best classifier we currently have\n",
    "\n",
    "From the classifier comparison notebook, the best model we currently have is the GaussianNaiveBayes. Let's create and train this classifier and then use it to predict probabilities and a label per observation in the testData.txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lu/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c58953ecbe2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprob_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predictions.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lu/anaconda3/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_log_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lu/anaconda3/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict_log_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mappear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;31m# normalize by P(x) = P(f_1, ..., f_n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mlog_prob_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lu/anaconda3/lib/python3.5/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"classes_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0mjoint_log_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lu/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;31m# make sure we acually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"O\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "clf=GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "prob_array = clf.predict_proba(testX[0:17377])\n",
    "\n",
    "with open('predictions.csv', 'w') as wp: \n",
    "    for row in prob_array: \n",
    "        index, value = max(enumerate(row), key=operator.itemgetter(1))\n",
    "        line_list = [\"%.3f\" % x for x in row] + [index+1] \n",
    "        line_str = '\\t'.join(line_list)+'\\n'\n",
    "        wp.write(line_str)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
