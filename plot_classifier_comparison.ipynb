{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classifier comparison\n",
    "\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and pre-processing data\n",
    "\n",
    "To begin with, I replaced NA's with zeros. \n",
    "\n",
    "Later, I will see how many features were mostly \"NA\". If they are not important, I will probably drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "trainX = [] \n",
    "with open(\"/home/lu/Documents/School/81_machine_learning/HW3/HW3/trainingData.txt\", 'r') as rp: \n",
    "    for line in rp.read().split(\"\\n\"): \n",
    "        row = line.split(\"\\t\") \n",
    "        i = 0 \n",
    "        for cell in row: \n",
    "            try: \n",
    "                row[i] = float(cell)\n",
    "            except ValueError: \n",
    "                row[i] = 0 \n",
    "            i+=1 \n",
    "        trainX.append(row)\n",
    "        \n",
    "trainY = [] \n",
    "with open(\"/home/lu/Documents/School/81_machine_learning/HW3/HW3/trainingTruth.txt\", 'r') as rp: \n",
    "    for line in rp.read().split(\"\\n\"): \n",
    "        trainY.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "### Split data into training and tests sets \n",
    "arrayX = np.array(trainX[0:3000])\n",
    "arrayY = np.array(trainY[0:3000])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(arrayX, arrayY, test_size=0.4, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Loop through all classifiers: \n",
    "\n",
    "classifiers = {\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(3),\n",
    "    \"Linear SVC\": SVC(kernel=\"linear\", C=0.5),\n",
    "    \"RBF SVM\": SVC(gamma=2, C=1),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=5),\n",
    "    \"Rand Forest\": RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \"Adaboost\": AdaBoostClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    #\"BernoulliNB\": BernoulliNB(), \n",
    "    #\"MultinomialNB\": MultinomialNB(),\n",
    "    \"Logit\": LogisticRegression(C=1), \n",
    "    \"Logit l=2\": LogisticRegression(C=0.5), \n",
    "    \"Logit l=4\": LogisticRegression(C=0.25), \n",
    "    \"QDA\": QuadraticDiscriminantAnalysis()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for high bias \n",
    "\n",
    "If the above accuracy on test is low, then it could be high bias or high variance. \n",
    "\n",
    "To score how we do on training will tell us if we have high bias. \n",
    "\n",
    "Scoring on test and then training shows the following. Buckets are less than 0.7 for high error, 0.7 to 0.8 for moderate error, greater than 0.8 for low error. \n",
    "\n",
    "\n",
    "| low bias (0.8+ on training)  | moderate bias (0.7 to 0.8 on training) | high bias (0.7- on training) | |\n",
    "| ------------- |:-------------:| ----------:| ------------------:|\n",
    "|               |  |      | **low variance (  0.7+ on test)**     |\n",
    "| Linear SVM     | Logit, NB   |       | **moderate variance  (0.5-0.7 on test)** |\n",
    "| QDA, RBF SVM |      | Adaboost, DT, RF    | **high variance (0.5- on test)**    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors accuracy on test: 0.565833\n",
      "Nearest Neighbors accuracy on training: 0.704444 \n",
      "\n",
      "QDA accuracy on test: 0.368333\n",
      "QDA accuracy on training: 1.000000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lu/anaconda3/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost accuracy on test: 0.574167\n",
      "Adaboost accuracy on training: 0.670000 \n",
      "\n",
      "DecisionTree accuracy on test: 0.425833\n",
      "DecisionTree accuracy on training: 0.570556 \n",
      "\n",
      "Logit accuracy on test: 0.681667\n",
      "Logit accuracy on training: 0.875000 \n",
      "\n",
      "GaussianNB accuracy on test: 0.760000\n",
      "GaussianNB accuracy on training: 0.841667 \n",
      "\n",
      "Rand Forest accuracy on test: 0.405000\n",
      "Rand Forest accuracy on training: 0.511667 \n",
      "\n",
      "RBF SVM accuracy on test: 0.368333\n",
      "RBF SVM accuracy on training: 1.000000 \n",
      "\n",
      "Logit l=4 accuracy on test: 0.683333\n",
      "Logit l=4 accuracy on training: 0.853889 \n",
      "\n",
      "Logit l=2 accuracy on test: 0.681667\n",
      "Logit l=2 accuracy on training: 0.861667 \n",
      "\n",
      "Linear SVC accuracy on test: 0.645833\n",
      "Linear SVC accuracy on training: 0.970000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Check bias and variance \n",
    "\n",
    "for name, classifier in  classifiers.items() : \n",
    "    clf=classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"%s accuracy on test: %f\" % (name, clf.score(X_test, y_test) ))\n",
    "    print(\"%s accuracy on training: %f \\n\" % (name, clf.score(X_train, y_train) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation to get more accurate assessment of variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors accuracy: 0.58 (+/- 0.04)\n",
      "QDA accuracy: 0.38 (+/- 0.02)\n",
      "Adaboost accuracy: 0.57 (+/- 0.05)\n",
      "DecisionTree accuracy: 0.45 (+/- 0.04)\n",
      "Logit accuracy: 0.69 (+/- 0.06)\n",
      "GaussianNB accuracy: 0.75 (+/- 0.07)\n",
      "Rand Forest accuracy: 0.40 (+/- 0.02)\n",
      "RBF SVM accuracy: 0.36 (+/- 0.00)\n",
      "Logit l=4 accuracy: 0.69 (+/- 0.05)\n",
      "Logit l=2 accuracy: 0.69 (+/- 0.05)\n",
      "Linear SVC accuracy: 0.66 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "for name, classifier in  classifiers.items() : \n",
    "    clf=classifier\n",
    "    scores = cross_val_score(clf, arrayX, arrayY, cv=10)\n",
    "    print(\"%s accuracy: %0.2f (+/- %0.2f)\" % (name, scores.mean(), scores.std() * 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit feature selection\n",
    "\n",
    "Try a few ways. First, see if decision tree and random forest can pick out some important features. \n",
    "\n",
    "Then, see if they agree with logit's feature weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "DT accuracy on test: 0.424167\n",
      "RF accuracy on test: 0.418333\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "# Feature importance -> a dictionary for later use, possibly to help me select features \n",
    "\n",
    "names = [\"DT\", \"RF\"]\n",
    "feature_importances = { \"DT\": [] , \"RF\": [] }\n",
    "important_indexes = { \"DT\": [] , \"RF\": [] }\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "]\n",
    "\n",
    "print(len(X_train))\n",
    "\n",
    "for name, classifier in zip(names, classifiers): \n",
    "    clf=classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"%s accuracy on test: %f\" % (name, clf.score(X_test, y_test) ))\n",
    "    feature_importances[name]=clf.feature_importances_\n",
    "\n",
    "for name, importances in feature_importances.items(): \n",
    "    features = list(enumerate(importances))\n",
    "    important_indexes[name] = [i for i, v in features if v > 0]\n",
    "\n",
    "dtX=arrayX[:,important_indexes['DT']]\n",
    "rfX=arrayX[:,important_indexes['RF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier | Score on Test | Score on Train | CV on all data | CV on DT features | CV on RF features\n",
      "--- | --- | --- | --- | --- | --- | --- | \n",
      "Nearest Neighbors|0.566|0.704|0.578|0.035|0.468|0.059|0.499|0.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lu/anaconda3/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA|0.368|1.000|0.385|0.019|0.549|0.029|0.490|0.029\n",
      "Adaboost|0.574|0.670|0.572|0.046|0.554|0.036|0.559|0.036\n",
      "DecisionTree|0.426|0.571|0.452|0.038|0.442|0.042|0.426|0.042\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifiers = {\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(3),\n",
    "    \"Linear SVC\": SVC(kernel=\"linear\", C=0.5),\n",
    "    \"RBF SVM\": SVC(gamma=2, C=1),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=5),\n",
    "    \"Rand Forest\": RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \"Adaboost\": AdaBoostClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"Logit\": LogisticRegression(C=1), \n",
    "    \"Logit l=2\": LogisticRegression(C=0.5), \n",
    "    \"Logit l=4\": LogisticRegression(C=0.25), \n",
    "    \"QDA\": QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "print(\"Classifier | Score on Test | Score on Train | CV on all data | CV on DT features | CV on RF features\")\n",
    "print(\"--- | --- | --- | --- | --- | --- | --- | \")\n",
    "\n",
    "for name, classifier in  classifiers.items() : \n",
    "    clf=classifier\n",
    "    scores = cross_val_score(clf, arrayX, arrayY, cv=10)\n",
    "    dt_scores = cross_val_score(clf, dtX, arrayY, cv=10)\n",
    "    rf_scores = cross_val_score(clf, rfX, arrayY, cv=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    row = [\n",
    "            clf.score(X_test, y_test) ,clf.score(X_train, y_train), #comparing test and training on 4/6 split\n",
    "            scores.mean(), scores.std() * 2, #comparing cross validated scores \n",
    "            dt_scores.mean(), dt_scores.std() * 2, # using dt thinnned data\n",
    "            rf_scores.mean(), dt_scores.std() * 2, # using rf thinned data \n",
    "         ]\n",
    "    print(name + \"|\" + \"|\".join([\"%.3f\" % x for x in row]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results after thinning variables according to tree feature importance \n",
    "\n",
    "\n",
    "Classifier | Score on Test | Score on Train | CV on all data | error | CV on DT features | error | CV on RF features | error\n",
    "--- | --- | --- | --- | --- | --- | --- |  --- | --- | \n",
    "NearestNeighbors|0.565833333333|0.704444444444|0.577691161968|0.03508060174|0.467939945369|0.059477632684|0.499345452353|0.059477632684\n",
    "QDA|0.368333333333|1.0|0.384672401604|0.0193418185832|0.548980185519|0.0294694432742|0.489634310008|0.0294694432742\n",
    "Adaboost|0.574166666667|0.67|0.572308722526|0.046208040059|0.554306985154|0.0364691224832|0.558999582936|0.0364691224832\n",
    "DecisionTree|0.425833333333|0.570555555556|0.452674113892|0.0380910086511|0.441619515653|0.0422924095744|0.426033432088|0.0422924095744\n",
    "Logit|0.681666666667|0.875|0.693667681574|0.0552929217141|0.58599765003|0.033998966029|0.658649913935|0.033998966029\n",
    "GaussianNB|0.76|0.841666666667|0.747586879902|0.0665484378079|0.590301185557|0.0505559355188|0.683608335048|0.0505559355188\n",
    "Rand Forest|0.410833333333|0.462777777778|0.395681838913|0.0391783398321|0.460050160459|0.0414290518191|0.397315015645|0.0414290518191\n",
    "RBF SVM|0.368333333333|1.0|0.358335115471|0.00158519469426|0.358335115471|0.00158519469426|0.358335115471|0.00158519469426\n",
    "Logit l=4|0.683333333333|0.853888888889|0.690330907607|0.0526520040119|0.584987616213|0.0412230092051|0.650284205201|0.0412230092051\n",
    "Logit l=2|0.681666666667|0.861666666667|0.693338778438|0.052499985639|0.587999939094|0.0365701288502|0.657289964605|0.0365701288502\n",
    "Linear SVC|0.645833333333|0.97|0.659352396568|0.0535692556328|0.582683128533|0.0466345803853|0.641979354172|0.0466345803853"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
