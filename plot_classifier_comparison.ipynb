{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classifier comparison\n",
    "\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and pre-processing data\n",
    "\n",
    "To begin with, I replaced NA's with zeros. \n",
    "\n",
    "Later, I will see how many features were mostly \"NA\". If they are not important, I will probably drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "trainX = [] \n",
    "with open(\"/home/lu/Documents/School/81_machine_learning/HW3/HW3/trainingData.txt\", 'r') as rp: \n",
    "    for line in rp.read().split(\"\\n\"): \n",
    "        row = line.split(\"\\t\") \n",
    "        i = 0 \n",
    "        for cell in row: \n",
    "            try: \n",
    "                row[i] = float(cell)\n",
    "            except ValueError: \n",
    "                row[i] = 0 \n",
    "            i+=1 \n",
    "        trainX.append(row)\n",
    "        \n",
    "trainY = [] \n",
    "with open(\"/home/lu/Documents/School/81_machine_learning/HW3/HW3/trainingTruth.txt\", 'r') as rp: \n",
    "    for line in rp.read().split(\"\\n\"): \n",
    "        trainY.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes cross validation: 0.768823529412\n"
     ]
    }
   ],
   "source": [
    "### Naive bayes classifier\n",
    "arrayX = np.array(trainX[0:17000])\n",
    "arrayY = np.array(trainY[0:17000])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(arrayX, arrayY, test_size=0.4, random_state=0)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#GaussianNB(priors=None)\n",
    "\n",
    "\n",
    "# check accuracy (probs won't be very accurate)\n",
    "print(\"Gaussian Naive Bayes cross validation:\" , clf.score(X_test, y_test)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy on test: 0.466176\n",
      "DT accuracy on training: 0.503627\n",
      "RF accuracy on test: 0.459853\n",
      "RF accuracy on training: 0.490490\n",
      "Gauss NB accuracy on test: 0.768824\n",
      "Gauss NB accuracy on training: 0.792157\n",
      "Logit accuracy on test: 0.756176\n",
      "Logit accuracy on training: 0.796667\n",
      "Logit l=2 accuracy on test: 0.755441\n",
      "Logit l=2 accuracy on training: 0.795392\n",
      "Logit l=4Logit CV accuracy on test: 0.752647\n",
      "Logit l=4Logit CV accuracy on training: 0.792843\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Loop through all classifiers: \n",
    "_names = [\n",
    "        \"Nearest Neighbors\", \n",
    "         \"Linear SVM\", \n",
    "         \"RBF SVM\",\n",
    "         \"Decision Tree\", \n",
    "         \"Random Forest\", \n",
    "         \"Adaboost\", \n",
    "         \"Naive Bayes\", \n",
    "         \"QDA\", \n",
    "    ]\n",
    "names = [\"DT\", \"RF\", \"Gauss NB\", \"Logit\", \"Logit l=2\", \"Logit l=4\" \"Logit CV\"]\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(3),\n",
    "    #SVC(kernel=\"linear\", C=0.025),\n",
    "    #SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    #AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    #BernoulliNB(), \n",
    "    #MultinomialNB(),\n",
    "    LogisticRegression(C=1), \n",
    "    LogisticRegression(C=0.5), \n",
    "    LogisticRegression(C=0.25), \n",
    "    LogisticRegressionCV(), \n",
    "    #QuadraticDiscriminantAnalysis()\n",
    "]\n",
    "\n",
    "for name, classifier in zip(names, classifiers): \n",
    "    clf=classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"%s accuracy on test: %f\" % (name, clf.score(X_test, y_test) ))\n",
    "    print(\"%s accuracy on training: %f\" % (name, clf.score(X_train, y_train) ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for high bias \n",
    "\n",
    "If the above accuracy on test is low, then it could be high bias or high variance. \n",
    "\n",
    "To score how we do on training will tell us if we have high bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, classifier in zip(names, classifiers): \n",
    "    clf=classifier\n",
    "    scores = cross_val_score(clf, arrayX, arrayY, cv=10)\n",
    "    print(\"%s accuracy: %0.2f (+/- %0.2f)\" % (name, scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit feature selection\n",
    "\n",
    "Try a few ways. First, see if decision tree and random forest can pick out some important features. \n",
    "\n",
    "Then, see if they agree with logit's feature weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy on test: 0.466176\n",
      "RF accuracy on test: 0.468382\n",
      "DT [15, 45, 47, 69, 78, 88, 104, 108, 114, 123, 142, 146, 148, 163, 211, 235, 243, 259, 264, 268, 279, 298, 303, 325]\n",
      "RF [0, 4, 7, 8, 9, 10, 14, 15, 16, 17, 18, 20, 21, 22, 25, 28, 31, 32, 33, 35, 41, 42, 43, 45, 46, 48, 49, 51, 52, 53, 54, 55, 58, 60, 61, 63, 64, 66, 67, 68, 75, 76, 79, 81, 83, 84, 85, 89, 90, 92, 96, 97, 99, 102, 105, 106, 108, 109, 110, 111, 113, 114, 115, 116, 117, 119, 120, 122, 123, 129, 133, 134, 135, 141, 142, 143, 144, 145, 151, 154, 155, 156, 158, 159, 160, 162, 163, 164, 165, 166, 169, 171, 173, 174, 176, 177, 178, 180, 181, 183, 186, 189, 191, 194, 195, 196, 198, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 216, 217, 219, 220, 221, 224, 225, 231, 233, 235, 239, 240, 242, 244, 246, 248, 250, 252, 254, 258, 260, 261, 263, 265, 266, 268, 269, 270, 272, 274, 275, 276, 277, 279, 281, 282, 283, 286, 288, 289, 290, 293, 294, 295, 298, 299, 303, 305, 306, 307, 308, 312, 313, 314, 315, 317, 318, 320, 323, 324, 325, 327, 328, 329]\n"
     ]
    }
   ],
   "source": [
    "# Feature imoprtance \n",
    "\n",
    "names = [\"DT\", \"RF\"]\n",
    "feature_importances = { \"DT\": [] , \"RF\": [] }\n",
    "important_indexes = { \"DT\": [] , \"RF\": [] }\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "]\n",
    "\n",
    "for name, classifier in zip(names, classifiers): \n",
    "    clf=classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"%s accuracy on test: %f\" % (name, clf.score(X_test, y_test) ))\n",
    "    feature_importances[name]=clf.feature_importances_\n",
    "\n",
    "for name, importances in feature_importances.items(): \n",
    "    features = list(enumerate(importances))\n",
    "    important_indexes[name] = [i for i, v in features if v > 0]\n",
    "    print(name, important_indexes[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 0), (2, 0), (3, 2), (4, 2)]\n"
     ]
    }
   ],
   "source": [
    "l = [1, 0, 0 , 2, 2]\n",
    "print(list(enumerate(l)))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
