{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classifier comparison\n",
    "\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "#from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and pre-processing data\n",
    "\n",
    "To begin with, I replaced NA's with zeros. \n",
    "\n",
    "Later, I will see how many features were mostly \"NA\". If they are not important, I will probably drop them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "trainX = [] \n",
    "with open(\"/home/lu/Documents/School/81_machine_learning/HW3/HW3/trainingData.txt\", 'r') as rp: \n",
    "    for line in rp.read().split(\"\\n\"): \n",
    "        row = line.split(\"\\t\") \n",
    "        i = 0 \n",
    "        for cell in row: \n",
    "            try: \n",
    "                row[i] = float(cell)\n",
    "            except ValueError: \n",
    "                row[i] = 0 \n",
    "            i+=1 \n",
    "        trainX.append(row)\n",
    "        \n",
    "trainY = [] \n",
    "with open(\"/home/lu/Documents/School/81_machine_learning/HW3/HW3/trainingTruth.txt\", 'r') as rp: \n",
    "    for line in rp.read().split(\"\\n\"): \n",
    "        trainY.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Split data into training and tests sets \n",
    "arrayX = np.array(trainX[0:17000])\n",
    "arrayY = np.array(trainY[0:17000])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(arrayX, arrayY, test_size=0.4, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Loop through all classifiers: \n",
    "\n",
    "classifiers = {\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(3),\n",
    "    \"Linear SVC\": SVC(kernel=\"linear\", C=0.5),\n",
    "    \"RBF SVM\": SVC(gamma=2, C=1),\n",
    "    \"DecisionTree_old\": DecisionTreeClassifier(max_depth=5), \n",
    "    \"RandomForest_old\": RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=20, random_state=0),\n",
    "    \"Rand Forest\": RandomForestClassifier(n_estimators=30, criterion='entropy', max_depth=20, min_samples_leaf=20, \n",
    "                           bootstrap=True, oob_score=False, random_state=0 ),\n",
    "    \"Adaboost\": AdaBoostClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"Logit\": LogisticRegression(C=1), \n",
    "    \"Logit l=2\": LogisticRegression(C=0.5), \n",
    "    \"Logit l=4\": LogisticRegression(C=0.25),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "print(\"Classifier | Score on Test | Score on Train \")\n",
    "print(\"--- | --- | --- | \")\n",
    "\n",
    "\n",
    "for name, classifier in  classifiers.items() : \n",
    "    clf=classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    row = [\n",
    "            clf.score(X_test, y_test) ,clf.score(X_train, y_train), #comparing test and training on 4/6 split\n",
    "         ]\n",
    "    print(name + \"|\" + \"|\".join([\"%.3f\" % x for x in row]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier comparison\n",
    "Classifier | Score on Test | Score on Train \n",
    "--- | --- | --- | \n",
    "Nearest Neighbors|0.579|0.740\n",
    "QDA|0.556|1.000\n",
    "Adaboost|0.600|0.632\n",
    "DecisionTree_old|0.460|0.526\n",
    "DecisionTree|0.440|0.497\n",
    "Logit|0.739|0.800\n",
    "GaussianNB|0.756|0.799\n",
    "RandomForest_old|0.389|0.436\n",
    "Rand Forest|0.593|0.787\n",
    "RBF SVM|0.343|1.000\n",
    "Logit l=4|0.728|0.794\n",
    "Logit l=2|0.737|0.799\n",
    "Linear SVC|0.704|0.832\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for high bias \n",
    "\n",
    "If the above accuracy on test is low, then it could be high bias or high variance. \n",
    "\n",
    "To score how we do on training will tell us if we have high bias. \n",
    "\n",
    "Scoring on test and then training shows the following. Buckets are less than 0.7 for high error, 0.7 to 0.8 for moderate error, greater than 0.8 for low error. \n",
    "\n",
    "\n",
    "| low bias (0.8+ on training)  | moderate bias (0.7 to 0.8 on training) | high bias (0.7- on training) | |\n",
    "| ------------- |:-------------:| ----------:| ------------------:|\n",
    "|               |  |      | **low variance (  0.7+ on test)**     |\n",
    "| Linear SVM     | Logit, NB   |       | **moderate variance  (0.5-0.7 on test)** |\n",
    "| QDA, RBF SVM |      | Adaboost, DT, RF    | **high variance (0.5- on test)**    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation to get more accurate assessment of variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors accuracy: 0.58 (+/- 0.04)\n",
      "QDA accuracy: 0.38 (+/- 0.02)\n",
      "Adaboost accuracy: 0.57 (+/- 0.05)\n",
      "DecisionTree accuracy: 0.45 (+/- 0.04)\n",
      "Logit accuracy: 0.69 (+/- 0.06)\n",
      "GaussianNB accuracy: 0.75 (+/- 0.07)\n",
      "Rand Forest accuracy: 0.40 (+/- 0.02)\n",
      "RBF SVM accuracy: 0.36 (+/- 0.00)\n",
      "Logit l=4 accuracy: 0.69 (+/- 0.05)\n",
      "Logit l=2 accuracy: 0.69 (+/- 0.05)\n",
      "Linear SVC accuracy: 0.66 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "for name, classifier in  classifiers.items() : \n",
    "    clf=classifier\n",
    "    scores = cross_val_score(clf, arrayX, arrayY, cv=10)\n",
    "    print(\"%s accuracy: %0.2f (+/- %0.2f)\" % (name, scores.mean(), scores.std() * 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit feature selection\n",
    "\n",
    "Try a few ways. First, see if decision tree and random forest can pick out some important features. \n",
    "\n",
    "Then, see if they agree with logit's feature weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "DT accuracy on test: 0.425000\n",
      "RF accuracy on test: 0.417500\n",
      "DT2 accuracy on test: 0.432500\n",
      "RF2 accuracy on test: 0.568333\n"
     ]
    }
   ],
   "source": [
    "# Feature importance -> a dictionary for later use, possibly to help me select features \n",
    "\n",
    "names = [\"DT\", \"RF\", \"DT2\", \"RF2\"]\n",
    "feature_importances = { \"DT\": [] , \"RF\": [] }\n",
    "important_indexes = { \"DT\": [] , \"RF\": [] }\n",
    "\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    DecisionTreeClassifier(criterion='entropy', max_depth=5,\n",
    "                           min_samples_split=2, min_samples_leaf=20, \n",
    "                           max_features=None,\n",
    "                           random_state=0, \n",
    "                           ),\n",
    "    RandomForestClassifier(n_estimators=100, criterion='entropy',\n",
    "                           max_depth=30, min_samples_split=2, \n",
    "                           min_samples_leaf=20, \n",
    "                           max_features='auto',\n",
    "                           bootstrap=True, oob_score=False, random_state=0, \n",
    "                           )\n",
    "]\n",
    "\n",
    "print(len(X_train))\n",
    "\n",
    "for name, classifier in zip(names, classifiers): \n",
    "    clf=classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"%s accuracy on test: %f\" % (name, clf.score(X_test, y_test) ))\n",
    "    feature_importances[name]=clf.feature_importances_\n",
    "\n",
    "for name, importances in feature_importances.items(): \n",
    "    features = list(enumerate(importances))\n",
    "    important_indexes[name] = [i for i, v in features if v > 0]\n",
    "\n",
    "dtX=arrayX[:,important_indexes['DT']]\n",
    "rfX=arrayX[:,important_indexes['RF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier | Score on Test | Score on Train | CV on all data | CV on DT features | CV on RF features\n",
      "--- | --- | --- | --- | --- | --- | --- | \n",
      "Nearest Neighbors|0.566|0.704|0.578|0.035|0.468|0.059|0.499|0.059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lu/anaconda3/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA|0.368|1.000|0.385|0.019|0.549|0.029|0.490|0.029\n",
      "Adaboost|0.574|0.670|0.572|0.046|0.554|0.036|0.559|0.036\n",
      "DecisionTree|0.426|0.571|0.452|0.038|0.442|0.042|0.426|0.042\n",
      "Logit|0.682|0.875|0.694|0.055|0.586|0.034|0.659|0.034\n",
      "GaussianNB|0.760|0.842|0.748|0.067|0.590|0.051|0.684|0.051\n",
      "Rand Forest|0.418|0.482|0.396|0.025|0.465|0.046|0.391|0.046\n",
      "RBF SVM|0.368|1.000|0.358|0.002|0.358|0.002|0.358|0.002\n",
      "Logit l=4|0.683|0.854|0.690|0.053|0.585|0.041|0.650|0.041\n",
      "Logit l=2|0.682|0.862|0.693|0.052|0.588|0.037|0.657|0.037\n",
      "Linear SVC|0.646|0.970|0.659|0.054|0.583|0.047|0.642|0.047\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier | Score on Test | Score on Train | CV on all data | CV on DT features | CV on RF features\")\n",
    "print(\"--- | --- | --- | --- | --- | --- | --- | \")\n",
    "\n",
    "for name, classifier in  classifiers.items() : \n",
    "    clf=classifier\n",
    "    scores = cross_val_score(clf, arrayX, arrayY, cv=10)\n",
    "    dt_scores = cross_val_score(clf, dtX, arrayY, cv=10)\n",
    "    rf_scores = cross_val_score(clf, rfX, arrayY, cv=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    row = [\n",
    "            clf.score(X_test, y_test) ,clf.score(X_train, y_train), #comparing test and training on 4/6 split\n",
    "            scores.mean(), scores.std() * 2, #comparing cross validated scores \n",
    "            dt_scores.mean(), dt_scores.std() * 2, # using dt thinnned data\n",
    "            rf_scores.mean(), dt_scores.std() * 2, # using rf thinned data \n",
    "         ]\n",
    "    print(name + \"|\" + \"|\".join([\"%.3f\" % x for x in row]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results after thinning variables according to tree feature importance \n",
    "\n",
    "To speed things up, I\"ve been using 3K of the given dataset. \n",
    "\n",
    "Classifier | Score on Test | Score on Train | CV on all data | error | CV on DT features | error | CV on RF features | error\n",
    "--- | --- | --- | --- | --- | --- | --- |  --- | --- | \n",
    "Nearest Neighbors|0.566|0.704|0.578|0.035|0.468|0.059|0.499|0.059\n",
    "QDA|0.368|1.000|0.385|0.019|0.549|0.029|0.490|0.029\n",
    "Adaboost|0.574|0.670|0.572|0.046|0.554|0.036|0.559|0.036\n",
    "DecisionTree|0.426|0.571|0.452|0.038|0.442|0.042|0.426|0.042\n",
    "Logit|0.682|0.875|0.694|0.055|0.586|0.034|0.659|0.034\n",
    "GaussianNB|0.760|0.842|0.748|0.067|0.590|0.051|0.684|0.051\n",
    "Rand Forest|0.418|0.482|0.396|0.025|0.465|0.046|0.391|0.046\n",
    "RBF SVM|0.368|1.000|0.358|0.002|0.358|0.002|0.358|0.002\n",
    "Logit l=4|0.683|0.854|0.690|0.053|0.585|0.041|0.650|0.041\n",
    "Logit l=2|0.682|0.862|0.693|0.052|0.588|0.037|0.657|0.037\n",
    "Linear SVC|0.646|0.970|0.659|0.054|0.583|0.047|0.642|0.047\n",
    "\n",
    "\n",
    "So it looks like this is not a good way to select features, though it seemed to help the Random Forest to make selections on fewer features. For my next run, I'm going to let the trees go deeper. The first time was using depth=5, which is not optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection - dropping mostly empty columns or rows \n",
    "\n",
    "For this data, we have filled NA's with zeros. Perhaps that is not good for some columns that contain legitimate small numbers. Dropping features that are mostly populated with NAs might improve things. \n",
    "\n",
    "Similarly, we can drop rows of data that contain too many NA's to be useful. From running this model on subsets of the data, a training set of 6K of the data creates a model that is nearly as good as a model that used 10K of the data (splitting the entire dataset). Therefore, we might be able to drop observations without reducing our dataset to a point where the model accuracy suffered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "some_classifiers = {\n",
    "    \"Linear SVC\": SVC(kernel=\"linear\", C=0.5),\n",
    "    \"Rand Forest\": RandomForestClassifier(n_estimators=30, criterion='entropy', max_depth=20, min_samples_leaf=20, \n",
    "                           bootstrap=True, oob_score=False, random_state=0 ),\n",
    "    \"Adaboost\": AdaBoostClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"Logit\": LogisticRegression(C=1), \n",
    "}\n",
    "\n",
    "shortX, shortY = arrayX[0:10000], arrayY[0:10000]\n",
    "sX_train, sX_test, sy_train, sy_test = train_test_split(shortX, shortY, test_size=0.4, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier | 7K Test | 10K Train |  4K Test | 6K Train \n",
      "--- | --- | --- | \n",
      "Rand Forest|0.64058824|0.67480392|0.59300000|0.78666667\n",
      "Adaboost|0.60500000|0.62098039|0.59950000|0.63200000\n",
      "Logit|0.76147059|0.76490196|0.73925000|0.80016667\n",
      "GaussianNB|0.77338235|0.78049020|0.75650000|0.79950000\n",
      "Linear SVC|0.74602941|0.76441176|0.70425000|0.83200000\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier | 7K Test | 10K Train |  4K Test | 6K Train \")\n",
    "print(\"--- | --- | --- | \")\n",
    "\n",
    "for name, classifier in  some_classifiers.items() : \n",
    "    clf=classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    short_clf = classifier\n",
    "    short_clf.fit(sX_train, sy_train)\n",
    "    row = [\n",
    "            clf.score(X_test, y_test) ,clf.score(X_train, y_train), #comparing test and training on 4/6 split\n",
    "            short_clf.score(sX_test, sy_test) ,short_clf.score(sX_train, sy_train)\n",
    "         ]\n",
    "    print(name + \"|\" + \"|\".join([\"%.8f\" % x for x in row]) )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
